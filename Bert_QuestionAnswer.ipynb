{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from IPython.core.debugger import set_trace\n",
    "import collections\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import modeling\n",
    "import optimization\n",
    "import tokenization\n",
    "import six\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import logging\n",
    "logging.getLogger('tensorflow').disabled = True\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "from pandas import Series\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import gensim.downloader as api\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")  # load pre-trained word-vectors from gensim-data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# flags = tf.flags\n",
    "\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "# ## Required parameters\n",
    "# flags.DEFINE_string(\n",
    "#     \"bert_config_file\", None,\n",
    "#     \"The config json file corresponding to the pre-trained BERT model. \"\n",
    "#     \"This specifies the model architecture.\")\n",
    "\n",
    "# flags.DEFINE_string(\"vocab_file\", None,\n",
    "#                     \"The vocabulary file that the BERT model was trained on.\")\n",
    "\n",
    "# flags.DEFINE_string(\n",
    "#     \"output_dir\", None,\n",
    "#     \"The output directory where the model checkpoints will be written.\")\n",
    "\n",
    "# ## Other parameters\n",
    "# flags.DEFINE_string(\"train_file\", None,\n",
    "#                     \"SQuAD json for training. E.g., train-v1.1.json\")\n",
    "\n",
    "# flags.DEFINE_string(\n",
    "#     \"predict_file\", None,\n",
    "#     \"SQuAD json for predictions. E.g., dev-v1.1.json or test-v1.1.json\")\n",
    "\n",
    "# flags.DEFINE_string(\n",
    "#     \"init_checkpoint\", None,\n",
    "#     \"Initial checkpoint (usually from a pre-trained BERT model).\")\n",
    "\n",
    "# flags.DEFINE_bool(\n",
    "#     \"do_lower_case\", True,\n",
    "#     \"Whether to lower case the input text. Should be True for uncased \"\n",
    "#     \"models and False for cased models.\")\n",
    "\n",
    "# flags.DEFINE_integer(\n",
    "#     \"max_seq_length\", 384,\n",
    "#     \"The maximum total input sequence length after WordPiece tokenization. \"\n",
    "#     \"Sequences longer than this will be truncated, and sequences shorter \"\n",
    "#     \"than this will be padded.\")\n",
    "\n",
    "# flags.DEFINE_integer(\n",
    "#     \"doc_stride\", 128,\n",
    "#     \"When splitting up a long document into chunks, how much stride to \"\n",
    "#     \"take between chunks.\")\n",
    "\n",
    "# flags.DEFINE_integer(\n",
    "#     \"max_query_length\", 64,\n",
    "#     \"The maximum number of tokens for the question. Questions longer than \"\n",
    "#     \"this will be truncated to this length.\")\n",
    "\n",
    "# flags.DEFINE_bool(\"do_train\", False, \"Whether to run training.\")\n",
    "\n",
    "# flags.DEFINE_bool(\"do_predict\", False, \"Whether to run eval on the dev set.\")\n",
    "\n",
    "# flags.DEFINE_integer(\"train_batch_size\", 32, \"Total batch size for training.\")\n",
    "\n",
    "# flags.DEFINE_integer(\"predict_batch_size\", 8,\n",
    "#                      \"Total batch size for predictions.\")\n",
    "\n",
    "# flags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n",
    "\n",
    "# flags.DEFINE_float(\"num_train_epochs\", 3.0,\n",
    "#                    \"Total number of training epochs to perform.\")\n",
    "\n",
    "# flags.DEFINE_float(\n",
    "#     \"warmup_proportion\", 0.1,\n",
    "#     \"Proportion of training to perform linear learning rate warmup for. \"\n",
    "#     \"E.g., 0.1 = 10% of training.\")\n",
    "\n",
    "# flags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n",
    "#                      \"How often to save the model checkpoint.\")\n",
    "\n",
    "# flags.DEFINE_integer(\"iterations_per_loop\", 1000,\n",
    "#                      \"How many steps to make in each estimator call.\")\n",
    "\n",
    "# flags.DEFINE_integer(\n",
    "#     \"n_best_size\", 20,\n",
    "#     \"The total number of n-best predictions to generate in the \"\n",
    "#     \"nbest_predictions.json output file.\")\n",
    "\n",
    "# flags.DEFINE_integer(\n",
    "#     \"max_answer_length\", 30,\n",
    "#     \"The maximum length of an answer that can be generated. This is needed \"\n",
    "#     \"because the start and end predictions are not conditioned on one another.\")\n",
    "\n",
    "# flags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU/CPU.\")\n",
    "\n",
    "# tf.flags.DEFINE_string(\n",
    "#     \"tpu_name\", None,\n",
    "#     \"The Cloud TPU to use for training. This should be either the name \"\n",
    "#     \"used when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 \"\n",
    "#     \"url.\")\n",
    "\n",
    "# tf.flags.DEFINE_string(\n",
    "#     \"tpu_zone\", None,\n",
    "#     \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n",
    "#     \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "#     \"metadata.\")\n",
    "\n",
    "# tf.flags.DEFINE_string(\n",
    "#     \"gcp_project\", None,\n",
    "#     \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n",
    "#     \"specified, we will attempt to automatically detect the GCE project from \"\n",
    "#     \"metadata.\")\n",
    "\n",
    "# tf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n",
    "\n",
    "# flags.DEFINE_integer(\n",
    "#     \"num_tpu_cores\", 8,\n",
    "#     \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n",
    "\n",
    "# flags.DEFINE_bool(\n",
    "#     \"verbose_logging\", False,\n",
    "#     \"If true, all of the warnings related to data processing will be printed. \"\n",
    "#     \"A number of warnings are expected for a normal SQuAD evaluation.\")\n",
    "\n",
    "# flags.DEFINE_bool(\n",
    "#     \"version_2_with_negative\", False,\n",
    "#     \"If true, the SQuAD examples contain some that do not have an answer.\")\n",
    "\n",
    "# flags.DEFINE_float(\n",
    "#     \"null_score_diff_threshold\", 0.0,\n",
    "#     \"If null_score - best_non_null is greater than the threshold predict null.\")\n",
    "\n",
    "# tf.flags.DEFINE_bool(\n",
    "#     \"interact\", False,\n",
    "#     \"to Initiate interactive question and answers chatbot\")\n",
    "# tf.flags.DEFINE_string(\n",
    "#     \"context\", None,\n",
    "#     \"context file path of Text summary to feed the BERT model \")\n",
    "# tf.flags.DEFINE_string(\n",
    "#     \"question\", None,\n",
    "#     \"Questioning sentence for the Text summary  \")\n",
    "\n",
    "\n",
    "class Flags:\n",
    "    def __init__(self):\n",
    "        self.bert_config_file = r'BERT_LARGE_DIR/bert_config.json'\n",
    "        self.vocab_file = r'BERT_LARGE_DIR/vocab.txt'\n",
    "        self.output_dir = 'squad_base'\n",
    "        self.train_file = 'train-v1.2.json'\n",
    "        self.predict_file = 'dev-v1.4.json'\n",
    "        self.init_checkpoint = r'BERT_LARGE_DIR/bert_model.ckpt'\n",
    "        self.do_lower_case = True\n",
    "        self.max_seq_length = 384\n",
    "        self.doc_stride = 128\n",
    "        self.max_query_length = 64\n",
    "        self.do_train = False\n",
    "        self.do_predict = True\n",
    "        self.train_batch_size = 12\n",
    "        self.predict_batch_size = 8\n",
    "        self.learning_rate = 3e-5\n",
    "        self.num_train_epochs = 3\n",
    "        self.warmup_proportion = 0.1\n",
    "        self.save_checkpoints_steps = 1000\n",
    "        self.iterations_per_loop = 1000\n",
    "        self.n_best_size = 20\n",
    "        self.max_answer_length = 30\n",
    "        self.use_tpu =  False\n",
    "        self.tpu_name = None\n",
    "        self.tpu_zone = None\n",
    "        self.gcp_project = None\n",
    "        self.master = None\n",
    "        self.num_tpu_cores = 8\n",
    "        self.verbose_logging = False\n",
    "        self.version_2_with_negative = False\n",
    "        self.null_score_diff_threshold = 0.0\n",
    "        self.context = None\n",
    "        self.question = None\n",
    "        self.interact = True\n",
    "FLAGS = Flags()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class SquadExample(object):\n",
    "  \"\"\"A single training/test example for simple sequence classification.\n",
    "\n",
    "     For examples without an answer, the start and end position are -1.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               qas_id,\n",
    "               question_text,\n",
    "               doc_tokens,\n",
    "               orig_answer_text=None,\n",
    "               start_position=None,\n",
    "               end_position=None,\n",
    "               is_impossible=False):\n",
    "    self.qas_id = qas_id\n",
    "    self.question_text = question_text\n",
    "    self.doc_tokens = doc_tokens\n",
    "    self.orig_answer_text = orig_answer_text\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position\n",
    "    self.is_impossible = is_impossible\n",
    "\n",
    "  def __str__(self):\n",
    "    return self.__repr__()\n",
    "\n",
    "  def __repr__(self):\n",
    "    s = \"\"\n",
    "    s += \"qas_id: %s\" % (tokenization.printable_text(self.qas_id))\n",
    "    s += \", question_text: %s\" % (\n",
    "        tokenization.printable_text(self.question_text))\n",
    "    s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n",
    "    if self.start_position:\n",
    "      s += \", start_position: %d\" % (self.start_position)\n",
    "    if self.start_position:\n",
    "      s += \", end_position: %d\" % (self.end_position)\n",
    "    if self.start_position:\n",
    "      s += \", is_impossible: %r\" % (self.is_impossible)\n",
    "    return s\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "  \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               unique_id,\n",
    "               example_index,\n",
    "               doc_span_index,\n",
    "               tokens,\n",
    "               token_to_orig_map,\n",
    "               token_is_max_context,\n",
    "               input_ids,\n",
    "               input_mask,\n",
    "               segment_ids,\n",
    "               start_position=None,\n",
    "               end_position=None,\n",
    "               is_impossible=None):\n",
    "    self.unique_id = unique_id\n",
    "    self.example_index = example_index\n",
    "    self.doc_span_index = doc_span_index\n",
    "    self.tokens = tokens\n",
    "    self.token_to_orig_map = token_to_orig_map\n",
    "    self.token_is_max_context = token_is_max_context\n",
    "    self.input_ids = input_ids\n",
    "    self.input_mask = input_mask\n",
    "    self.segment_ids = segment_ids\n",
    "    self.start_position = start_position\n",
    "    self.end_position = end_position\n",
    "    self.is_impossible = is_impossible\n",
    "\n",
    "\n",
    "def read_squad_examples(data, is_training):\n",
    "  \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n",
    "#   with tf.gfile.Open(input_file, \"r\") as reader:\n",
    "#     input_data = json.load(reader)[\"data\"]\n",
    "  input_data = data['data']\n",
    "\n",
    "  def is_whitespace(c):\n",
    "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
    "      return True\n",
    "    return False\n",
    "\n",
    "  examples = []\n",
    "  for entry in input_data[:2]:\n",
    "    for paragraph in entry[\"paragraphs\"]:\n",
    "      paragraph_text = paragraph[\"context\"]\n",
    "      doc_tokens = []\n",
    "      char_to_word_offset = []\n",
    "      prev_is_whitespace = True\n",
    "      for c in paragraph_text:\n",
    "        if is_whitespace(c):\n",
    "          prev_is_whitespace = True\n",
    "        else:\n",
    "          if prev_is_whitespace:\n",
    "            doc_tokens.append(c)\n",
    "          else:\n",
    "            doc_tokens[-1] += c\n",
    "          prev_is_whitespace = False\n",
    "        char_to_word_offset.append(len(doc_tokens) - 1)\n",
    "\n",
    "      for qa in paragraph[\"qas\"]:\n",
    "        qas_id = qa[\"id\"]\n",
    "        question_text = qa[\"question\"]\n",
    "        start_position = None\n",
    "        end_position = None\n",
    "        orig_answer_text = None\n",
    "        is_impossible = False\n",
    "        if is_training:\n",
    "\n",
    "          if FLAGS.version_2_with_negative:\n",
    "            is_impossible = qa[\"is_impossible\"]\n",
    "          if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n",
    "            raise ValueError(\n",
    "                \"For training, each question should have exactly 1 answer.\")\n",
    "          if not is_impossible:\n",
    "            answer = qa[\"answers\"][0]\n",
    "            orig_answer_text = answer[\"text\"]\n",
    "            answer_offset = answer[\"answer_start\"]\n",
    "            answer_length = len(orig_answer_text)\n",
    "            start_position = char_to_word_offset[answer_offset]\n",
    "            end_position = char_to_word_offset[answer_offset + answer_length -\n",
    "                                               1]\n",
    "            # Only add answers where the text can be exactly recovered from the\n",
    "            # document. If this CAN'T happen it's likely due to weird Unicode\n",
    "            # stuff so we will just skip the example.\n",
    "            #\n",
    "            # Note that this means for training mode, every example is NOT\n",
    "            # guaranteed to be preserved.\n",
    "            actual_text = \" \".join(\n",
    "                doc_tokens[start_position:(end_position + 1)])\n",
    "            cleaned_answer_text = \" \".join(\n",
    "                tokenization.whitespace_tokenize(orig_answer_text))\n",
    "            if actual_text.find(cleaned_answer_text) == -1:\n",
    "#               tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
    "#                                  actual_text, cleaned_answer_text)\n",
    "              continue\n",
    "          else:\n",
    "            start_position = -1\n",
    "            end_position = -1\n",
    "            orig_answer_text = \"\"\n",
    "\n",
    "        example = SquadExample(\n",
    "            qas_id=qas_id,\n",
    "            question_text=question_text,\n",
    "            doc_tokens=doc_tokens,\n",
    "            orig_answer_text=orig_answer_text,\n",
    "            start_position=start_position,\n",
    "            end_position=end_position,\n",
    "            is_impossible=is_impossible)\n",
    "        examples.append(example)\n",
    "\n",
    "  return examples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples, tokenizer, max_seq_length,\n",
    "                                 doc_stride, max_query_length, is_training,\n",
    "                                 output_fn):\n",
    "  \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "  unique_id = 1000000000\n",
    "\n",
    "  for (example_index, example) in enumerate(examples):\n",
    "    query_tokens = tokenizer.tokenize(example.question_text)\n",
    "\n",
    "    if len(query_tokens) > max_query_length:\n",
    "      query_tokens = query_tokens[0:max_query_length]\n",
    "\n",
    "    tok_to_orig_index = []\n",
    "    orig_to_tok_index = []\n",
    "    all_doc_tokens = []\n",
    "    for (i, token) in enumerate(example.doc_tokens):\n",
    "      orig_to_tok_index.append(len(all_doc_tokens))\n",
    "      sub_tokens = tokenizer.tokenize(token)\n",
    "      for sub_token in sub_tokens:\n",
    "        tok_to_orig_index.append(i)\n",
    "        all_doc_tokens.append(sub_token)\n",
    "\n",
    "    tok_start_position = None\n",
    "    tok_end_position = None\n",
    "    if is_training and example.is_impossible:\n",
    "      tok_start_position = -1\n",
    "      tok_end_position = -1\n",
    "    if is_training and not example.is_impossible:\n",
    "      tok_start_position = orig_to_tok_index[example.start_position]\n",
    "      if example.end_position < len(example.doc_tokens) - 1:\n",
    "        tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n",
    "      else:\n",
    "        tok_end_position = len(all_doc_tokens) - 1\n",
    "      (tok_start_position, tok_end_position) = _improve_answer_span(\n",
    "          all_doc_tokens, tok_start_position, tok_end_position, tokenizer,\n",
    "          example.orig_answer_text)\n",
    "\n",
    "    # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "    # We can have documents that are longer than the maximum sequence length.\n",
    "    # To deal with this we do a sliding window approach, where we take chunks\n",
    "    # of the up to our max length with a stride of `doc_stride`.\n",
    "    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"DocSpan\", [\"start\", \"length\"])\n",
    "    doc_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < len(all_doc_tokens):\n",
    "      length = len(all_doc_tokens) - start_offset\n",
    "      if length > max_tokens_for_doc:\n",
    "        length = max_tokens_for_doc\n",
    "      doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "      if start_offset + length == len(all_doc_tokens):\n",
    "        break\n",
    "      start_offset += min(length, doc_stride)\n",
    "\n",
    "    for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "      tokens = []\n",
    "      token_to_orig_map = {}\n",
    "      token_is_max_context = {}\n",
    "      segment_ids = []\n",
    "      tokens.append(\"[CLS]\")\n",
    "      segment_ids.append(0)\n",
    "      for token in query_tokens:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "      tokens.append(\"[SEP]\")\n",
    "      segment_ids.append(0)\n",
    "\n",
    "      for i in range(doc_span.length):\n",
    "        split_token_index = doc_span.start + i\n",
    "        token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "        is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                               split_token_index)\n",
    "        token_is_max_context[len(tokens)] = is_max_context\n",
    "        tokens.append(all_doc_tokens[split_token_index])\n",
    "        segment_ids.append(1)\n",
    "      tokens.append(\"[SEP]\")\n",
    "      segment_ids.append(1)\n",
    "\n",
    "      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "      # tokens are attended to.\n",
    "      input_mask = [1] * len(input_ids)\n",
    "\n",
    "      # Zero-pad up to the sequence length.\n",
    "      while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "      assert len(input_ids) == max_seq_length\n",
    "      assert len(input_mask) == max_seq_length\n",
    "      assert len(segment_ids) == max_seq_length\n",
    "\n",
    "      start_position = None\n",
    "      end_position = None\n",
    "      if is_training and not example.is_impossible:\n",
    "        # For training, if our document chunk does not contain an annotation\n",
    "        # we throw it out, since there is nothing to predict.\n",
    "        doc_start = doc_span.start\n",
    "        doc_end = doc_span.start + doc_span.length - 1\n",
    "        out_of_span = False\n",
    "        if not (tok_start_position >= doc_start and\n",
    "                tok_end_position <= doc_end):\n",
    "          out_of_span = True\n",
    "        if out_of_span:\n",
    "          start_position = 0\n",
    "          end_position = 0\n",
    "        else:\n",
    "          doc_offset = len(query_tokens) + 2\n",
    "          start_position = tok_start_position - doc_start + doc_offset\n",
    "          end_position = tok_end_position - doc_start + doc_offset\n",
    "\n",
    "      if is_training and example.is_impossible:\n",
    "        start_position = 0\n",
    "        end_position = 0\n",
    "\n",
    "#       if example_index < 20:\n",
    "#         tf.logging.info(\"*** Example ***\")\n",
    "#         tf.logging.info(\"unique_id: %s\" % (unique_id))\n",
    "#         tf.logging.info(\"example_index: %s\" % (example_index))\n",
    "#         tf.logging.info(\"doc_span_index: %s\" % (doc_span_index))\n",
    "#         tf.logging.info(\"tokens: %s\" % \" \".join(\n",
    "#             [tokenization.printable_text(x) for x in tokens]))\n",
    "#         tf.logging.info(\"token_to_orig_map: %s\" % \" \".join(\n",
    "#             [\"%d:%d\" % (x, y) for (x, y) in six.iteritems(token_to_orig_map)]))\n",
    "#         tf.logging.info(\"token_is_max_context: %s\" % \" \".join([\n",
    "#             \"%d:%s\" % (x, y) for (x, y) in six.iteritems(token_is_max_context)\n",
    "#         ]))\n",
    "#         tf.logging.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "#         tf.logging.info(\n",
    "#             \"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "#         tf.logging.info(\n",
    "#             \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "#         if is_training and example.is_impossible:\n",
    "#           tf.logging.info(\"impossible example\")\n",
    "#         if is_training and not example.is_impossible:\n",
    "#           answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n",
    "#           tf.logging.info(\"start_position: %d\" % (start_position))\n",
    "#           tf.logging.info(\"end_position: %d\" % (end_position))\n",
    "#           tf.logging.info(\n",
    "#               \"answer: %s\" % (tokenization.printable_text(answer_text)))\n",
    "\n",
    "      feature = InputFeatures(\n",
    "          unique_id=unique_id,\n",
    "          example_index=example_index,\n",
    "          doc_span_index=doc_span_index,\n",
    "          tokens=tokens,\n",
    "          token_to_orig_map=token_to_orig_map,\n",
    "          token_is_max_context=token_is_max_context,\n",
    "          input_ids=input_ids,\n",
    "          input_mask=input_mask,\n",
    "          segment_ids=segment_ids,\n",
    "          start_position=start_position,\n",
    "          end_position=end_position,\n",
    "          is_impossible=example.is_impossible)\n",
    "\n",
    "      # Run callback\n",
    "      output_fn(feature)\n",
    "\n",
    "      unique_id += 1\n",
    "\n",
    "\n",
    "def _improve_answer_span(doc_tokens, input_start, input_end, tokenizer,\n",
    "                         orig_answer_text):\n",
    "  \"\"\"Returns tokenized answer spans that better match the annotated answer.\"\"\"\n",
    "\n",
    "  # The SQuAD annotations are character based. We first project them to\n",
    "  # whitespace-tokenized words. But then after WordPiece tokenization, we can\n",
    "  # often find a \"better match\". For example:\n",
    "  #\n",
    "  #   Question: What year was John Smith born?\n",
    "  #   Context: The leader was John Smith (1895-1943).\n",
    "  #   Answer: 1895\n",
    "  #\n",
    "  # The original whitespace-tokenized answer will be \"(1895-1943).\". However\n",
    "  # after tokenization, our tokens will be \"( 1895 - 1943 ) .\". So we can match\n",
    "  # the exact answer, 1895.\n",
    "  #\n",
    "  # However, this is not always possible. Consider the following:\n",
    "  #\n",
    "  #   Question: What country is the top exporter of electornics?\n",
    "  #   Context: The Japanese electronics industry is the lagest in the world.\n",
    "  #   Answer: Japan\n",
    "  #\n",
    "  # In this case, the annotator chose \"Japan\" as a character sub-span of\n",
    "  # the word \"Japanese\". Since our WordPiece tokenizer does not split\n",
    "  # \"Japanese\", we just use \"Japanese\" as the annotation. This is fairly rare\n",
    "  # in SQuAD, but does happen.\n",
    "  tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n",
    "\n",
    "  for new_start in range(input_start, input_end + 1):\n",
    "    for new_end in range(input_end, new_start - 1, -1):\n",
    "      text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n",
    "      if text_span == tok_answer_text:\n",
    "        return (new_start, new_end)\n",
    "\n",
    "  return (input_start, input_end)\n",
    "\n",
    "\n",
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "  \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "  # Because of the sliding window approach taken to scoring documents, a single\n",
    "  # token can appear in multiple documents. E.g.\n",
    "  #  Doc: the man went to the store and bought a gallon of milk\n",
    "  #  Span A: the man went to the\n",
    "  #  Span B: to the store and bought\n",
    "  #  Span C: and bought a gallon of\n",
    "  #  ...\n",
    "  #\n",
    "  # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "  # want to consider the score with \"maximum context\", which we define as\n",
    "  # the *minimum* of its left and right context (the *sum* of left and\n",
    "  # right context will always be the same, of course).\n",
    "  #\n",
    "  # In the example the maximum context for 'bought' would be span C since\n",
    "  # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "  # and 0 right context.\n",
    "  best_score = None\n",
    "  best_span_index = None\n",
    "  for (span_index, doc_span) in enumerate(doc_spans):\n",
    "    end = doc_span.start + doc_span.length - 1\n",
    "    if position < doc_span.start:\n",
    "      continue\n",
    "    if position > end:\n",
    "      continue\n",
    "    num_left_context = position - doc_span.start\n",
    "    num_right_context = end - position\n",
    "    score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "    if best_score is None or score > best_score:\n",
    "      best_score = score\n",
    "      best_span_index = span_index\n",
    "\n",
    "  return cur_span_index == best_span_index\n",
    "\n",
    "\n",
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
    "                 use_one_hot_embeddings):\n",
    "  \"\"\"Creates a classification model.\"\"\"\n",
    "  model = modeling.BertModel(\n",
    "      config=bert_config,\n",
    "      is_training=is_training,\n",
    "      input_ids=input_ids,\n",
    "      input_mask=input_mask,\n",
    "      token_type_ids=segment_ids,\n",
    "      use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "  final_hidden = model.get_sequence_output()\n",
    "\n",
    "  final_hidden_shape = modeling.get_shape_list(final_hidden, expected_rank=3)\n",
    "  batch_size = final_hidden_shape[0]\n",
    "  seq_length = final_hidden_shape[1]\n",
    "  hidden_size = final_hidden_shape[2]\n",
    "\n",
    "  output_weights = tf.get_variable(\n",
    "      \"cls/squad/output_weights\", [2, hidden_size],\n",
    "      initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "  output_bias = tf.get_variable(\n",
    "      \"cls/squad/output_bias\", [2], initializer=tf.zeros_initializer())\n",
    "\n",
    "  final_hidden_matrix = tf.reshape(final_hidden,\n",
    "                                   [batch_size * seq_length, hidden_size])\n",
    "  logits = tf.matmul(final_hidden_matrix, output_weights, transpose_b=True)\n",
    "  logits = tf.nn.bias_add(logits, output_bias)\n",
    "\n",
    "  logits = tf.reshape(logits, [batch_size, seq_length, 2])\n",
    "  logits = tf.transpose(logits, [2, 0, 1])\n",
    "\n",
    "  unstacked_logits = tf.unstack(logits, axis=0)\n",
    "\n",
    "  (start_logits, end_logits) = (unstacked_logits[0], unstacked_logits[1])\n",
    "\n",
    "  return (start_logits, end_logits)\n",
    "\n",
    "\n",
    "def model_fn_builder(bert_config, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "#     tf.logging.info(\"*** Features ***\")\n",
    "#     for name in sorted(features.keys()):\n",
    "#       tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "    unique_ids = features[\"unique_ids\"]\n",
    "    input_ids = features[\"input_ids\"]\n",
    "    input_mask = features[\"input_mask\"]\n",
    "    segment_ids = features[\"segment_ids\"]\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    (start_logits, end_logits) = create_model(\n",
    "        bert_config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    tvars = tf.trainable_variables()\n",
    "\n",
    "    initialized_variable_names = {}\n",
    "    scaffold_fn = None\n",
    "    if init_checkpoint:\n",
    "      (assignment_map, initialized_variable_names\n",
    "      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "      if use_tpu:\n",
    "\n",
    "        def tpu_scaffold():\n",
    "          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "          return tf.train.Scaffold()\n",
    "\n",
    "        scaffold_fn = tpu_scaffold\n",
    "      else:\n",
    "        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "#     tf.logging.info(\"**** Trainable Variables ****\")\n",
    "    for var in tvars:\n",
    "      init_string = \"\"\n",
    "      if var.name in initialized_variable_names:\n",
    "        init_string = \", *INIT_FROM_CKPT*\"\n",
    "#       tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n",
    "#                       init_string)\n",
    "\n",
    "    output_spec = None\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "      seq_length = modeling.get_shape_list(input_ids)[1]\n",
    "\n",
    "      def compute_loss(logits, positions):\n",
    "        one_hot_positions = tf.one_hot(\n",
    "            positions, depth=seq_length, dtype=tf.float32)\n",
    "        log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        loss = -tf.reduce_mean(\n",
    "            tf.reduce_sum(one_hot_positions * log_probs, axis=-1))\n",
    "        return loss\n",
    "\n",
    "      start_positions = features[\"start_positions\"]\n",
    "      end_positions = features[\"end_positions\"]\n",
    "\n",
    "      start_loss = compute_loss(start_logits, start_positions)\n",
    "      end_loss = compute_loss(end_logits, end_positions)\n",
    "\n",
    "      total_loss = (start_loss + end_loss) / 2.0\n",
    "\n",
    "      train_op = optimization.create_optimizer(\n",
    "          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode,\n",
    "          loss=total_loss,\n",
    "          train_op=train_op,\n",
    "          scaffold_fn=scaffold_fn)\n",
    "    elif mode == tf.estimator.ModeKeys.PREDICT:\n",
    "      predictions = {\n",
    "          \"unique_ids\": unique_ids,\n",
    "          \"start_logits\": start_logits,\n",
    "          \"end_logits\": end_logits,\n",
    "      }\n",
    "      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n",
    "          mode=mode, predictions=predictions, scaffold_fn=scaffold_fn)\n",
    "    else:\n",
    "      raise ValueError(\n",
    "          \"Only TRAIN and PREDICT modes are supported: %s\" % (mode))\n",
    "\n",
    "    return output_spec\n",
    "\n",
    "  return model_fn\n",
    "\n",
    "\n",
    "def input_fn_builder(input_file, seq_length, is_training, drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  name_to_features = {\n",
    "      \"unique_ids\": tf.FixedLenFeature([], tf.int64),\n",
    "      \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "      \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "  }\n",
    "\n",
    "  if is_training:\n",
    "    name_to_features[\"start_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
    "    name_to_features[\"end_positions\"] = tf.FixedLenFeature([], tf.int64)\n",
    "\n",
    "  def _decode_record(record, name_to_features):\n",
    "    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "    example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "    # So cast all int64 to int32.\n",
    "    for name in list(example.keys()):\n",
    "      t = example[name]\n",
    "      if t.dtype == tf.int64:\n",
    "        t = tf.to_int32(t)\n",
    "      example[name] = t\n",
    "\n",
    "    return example\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "\n",
    "    # For training, we want a lot of parallel reading and shuffling.\n",
    "    # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "    d = tf.data.TFRecordDataset(input_file)\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.apply(\n",
    "        tf.contrib.data.map_and_batch(\n",
    "            lambda record: _decode_record(record, name_to_features),\n",
    "            batch_size=batch_size,\n",
    "            drop_remainder=drop_remainder))\n",
    "\n",
    "    return d\n",
    "\n",
    "  return input_fn\n",
    "\n",
    "\n",
    "RawResult = collections.namedtuple(\"RawResult\",\n",
    "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
    "\n",
    "\n",
    "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
    "                      max_answer_length, do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file):\n",
    "  \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
    "#   tf.logging.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
    "#   tf.logging.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
    "\n",
    "  example_index_to_features = collections.defaultdict(list)\n",
    "  for feature in all_features:\n",
    "    example_index_to_features[feature.example_index].append(feature)\n",
    "\n",
    "  unique_id_to_result = {}\n",
    "  for result in all_results:\n",
    "    unique_id_to_result[result.unique_id] = result\n",
    "\n",
    "  _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "      \"PrelimPrediction\",\n",
    "      [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "  all_predictions = collections.OrderedDict()\n",
    "  all_nbest_json = collections.OrderedDict()\n",
    "  scores_diff_json = collections.OrderedDict()\n",
    "\n",
    "  for (example_index, example) in enumerate(all_examples):\n",
    "    features = example_index_to_features[example_index]\n",
    "\n",
    "    prelim_predictions = []\n",
    "    # keep track of the minimum score of null start+end of position 0\n",
    "    score_null = 1000000  # large and positive\n",
    "    min_null_feature_index = 0  # the paragraph slice with min mull score\n",
    "    null_start_logit = 0  # the start logit at the slice with min null score\n",
    "    null_end_logit = 0  # the end logit at the slice with min null score\n",
    "    for (feature_index, feature) in enumerate(features):\n",
    "      result = unique_id_to_result[feature.unique_id]\n",
    "      start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
    "      end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
    "      # if we could have irrelevant answers, get the min score of irrelevant\n",
    "      if FLAGS.version_2_with_negative:\n",
    "        feature_null_score = result.start_logits[0] + result.end_logits[0]\n",
    "        if feature_null_score < score_null:\n",
    "          score_null = feature_null_score\n",
    "          min_null_feature_index = feature_index\n",
    "          null_start_logit = result.start_logits[0]\n",
    "          null_end_logit = result.end_logits[0]\n",
    "      for start_index in start_indexes:\n",
    "        for end_index in end_indexes:\n",
    "          # We could hypothetically create invalid predictions, e.g., predict\n",
    "          # that the start of the span is in the question. We throw out all\n",
    "          # invalid predictions.\n",
    "          if start_index >= len(feature.tokens):\n",
    "            continue\n",
    "          if end_index >= len(feature.tokens):\n",
    "            continue\n",
    "          if start_index not in feature.token_to_orig_map:\n",
    "            continue\n",
    "          if end_index not in feature.token_to_orig_map:\n",
    "            continue\n",
    "          if not feature.token_is_max_context.get(start_index, False):\n",
    "            continue\n",
    "          if end_index < start_index:\n",
    "            continue\n",
    "          length = end_index - start_index + 1\n",
    "          if length > max_answer_length:\n",
    "            continue\n",
    "          prelim_predictions.append(\n",
    "              _PrelimPrediction(\n",
    "                  feature_index=feature_index,\n",
    "                  start_index=start_index,\n",
    "                  end_index=end_index,\n",
    "                  start_logit=result.start_logits[start_index],\n",
    "                  end_logit=result.end_logits[end_index]))\n",
    "\n",
    "    if FLAGS.version_2_with_negative:\n",
    "      prelim_predictions.append(\n",
    "          _PrelimPrediction(\n",
    "              feature_index=min_null_feature_index,\n",
    "              start_index=0,\n",
    "              end_index=0,\n",
    "              start_logit=null_start_logit,\n",
    "              end_logit=null_end_logit))\n",
    "    prelim_predictions = sorted(\n",
    "        prelim_predictions,\n",
    "        key=lambda x: (x.start_logit + x.end_logit),\n",
    "        reverse=True)\n",
    "\n",
    "    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
    "\n",
    "    seen_predictions = {}\n",
    "    nbest = []\n",
    "    for pred in prelim_predictions:\n",
    "      if len(nbest) >= n_best_size:\n",
    "        break\n",
    "      feature = features[pred.feature_index]\n",
    "      if pred.start_index > 0:  # this is a non-null prediction\n",
    "        tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
    "        orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
    "        orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
    "        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
    "        tok_text = \" \".join(tok_tokens)\n",
    "\n",
    "        # De-tokenize WordPieces that have been split off.\n",
    "        tok_text = tok_text.replace(\" ##\", \"\")\n",
    "        tok_text = tok_text.replace(\"##\", \"\")\n",
    "\n",
    "        # Clean whitespace\n",
    "        tok_text = tok_text.strip()\n",
    "        tok_text = \" \".join(tok_text.split())\n",
    "        orig_text = \" \".join(orig_tokens)\n",
    "\n",
    "        final_text = get_final_text(tok_text, orig_text, do_lower_case)\n",
    "        if final_text in seen_predictions:\n",
    "          continue\n",
    "\n",
    "        seen_predictions[final_text] = True\n",
    "      else:\n",
    "        final_text = \"\"\n",
    "        seen_predictions[final_text] = True\n",
    "\n",
    "      nbest.append(\n",
    "          _NbestPrediction(\n",
    "              text=final_text,\n",
    "              start_logit=pred.start_logit,\n",
    "              end_logit=pred.end_logit))\n",
    "\n",
    "    # if we didn't inlude the empty option in the n-best, inlcude it\n",
    "    if FLAGS.version_2_with_negative:\n",
    "      if \"\" not in seen_predictions:\n",
    "        nbest.append(\n",
    "            _NbestPrediction(\n",
    "                text=\"\", start_logit=null_start_logit,\n",
    "                end_logit=null_end_logit))\n",
    "    # In very rare edge cases we could have no valid predictions. So we\n",
    "    # just create a nonce prediction in this case to avoid failure.\n",
    "    if not nbest:\n",
    "      nbest.append(\n",
    "          _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
    "\n",
    "    assert len(nbest) >= 1\n",
    "\n",
    "    total_scores = []\n",
    "    best_non_null_entry = None\n",
    "    for entry in nbest:\n",
    "      total_scores.append(entry.start_logit + entry.end_logit)\n",
    "      if not best_non_null_entry:\n",
    "        if entry.text:\n",
    "          best_non_null_entry = entry\n",
    "\n",
    "    probs = _compute_softmax(total_scores)\n",
    "\n",
    "    nbest_json = []\n",
    "    for (i, entry) in enumerate(nbest):\n",
    "      output = collections.OrderedDict()\n",
    "      output[\"text\"] = entry.text\n",
    "      output[\"probability\"] = probs[i]\n",
    "      output[\"start_logit\"] = entry.start_logit\n",
    "      output[\"end_logit\"] = entry.end_logit\n",
    "      nbest_json.append(output)\n",
    "\n",
    "    assert len(nbest_json) >= 1\n",
    "\n",
    "    if not FLAGS.version_2_with_negative:\n",
    "      all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
    "    else:\n",
    "      # predict \"\" iff the null score - the score of best non-null > threshold\n",
    "      score_diff = score_null - best_non_null_entry.start_logit - (\n",
    "          best_non_null_entry.end_logit)\n",
    "      scores_diff_json[example.qas_id] = score_diff\n",
    "      if score_diff > FLAGS.null_score_diff_threshold:\n",
    "        all_predictions[example.qas_id] = \"\"\n",
    "      else:\n",
    "        all_predictions[example.qas_id] = best_non_null_entry.text\n",
    "\n",
    "    all_nbest_json[example.qas_id] = nbest_json\n",
    "  if FLAGS.interact:\n",
    "    return all_predictions\n",
    "  with tf.gfile.GFile(output_prediction_file, \"w\") as writer:\n",
    "    writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
    "\n",
    "  with tf.gfile.GFile(output_nbest_file, \"w\") as writer:\n",
    "    writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
    "\n",
    "  if FLAGS.version_2_with_negative:\n",
    "    with tf.gfile.GFile(output_null_log_odds_file, \"w\") as writer:\n",
    "      writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n",
    "\n",
    "\n",
    "def get_final_text(pred_text, orig_text, do_lower_case):\n",
    "  \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n",
    "\n",
    "  # When we created the data, we kept track of the alignment between original\n",
    "  # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n",
    "  # now `orig_text` contains the span of our original text corresponding to the\n",
    "  # span that we predicted.\n",
    "  #\n",
    "  # However, `orig_text` may contain extra characters that we don't want in\n",
    "  # our prediction.\n",
    "  #\n",
    "  # For example, let's say:\n",
    "  #   pred_text = steve smith\n",
    "  #   orig_text = Steve Smith's\n",
    "  #\n",
    "  # We don't want to return `orig_text` because it contains the extra \"'s\".\n",
    "  #\n",
    "  # We don't want to return `pred_text` because it's already been normalized\n",
    "  # (the SQuAD eval script also does punctuation stripping/lower casing but\n",
    "  # our tokenizer does additional normalization like stripping accent\n",
    "  # characters).\n",
    "  #\n",
    "  # What we really want to return is \"Steve Smith\".\n",
    "  #\n",
    "  # Therefore, we have to apply a semi-complicated alignment heruistic between\n",
    "  # `pred_text` and `orig_text` to get a character-to-charcter alignment. This\n",
    "  # can fail in certain cases in which case we just return `orig_text`.\n",
    "\n",
    "  def _strip_spaces(text):\n",
    "    ns_chars = []\n",
    "    ns_to_s_map = collections.OrderedDict()\n",
    "    for (i, c) in enumerate(text):\n",
    "      if c == \" \":\n",
    "        continue\n",
    "      ns_to_s_map[len(ns_chars)] = i\n",
    "      ns_chars.append(c)\n",
    "    ns_text = \"\".join(ns_chars)\n",
    "    return (ns_text, ns_to_s_map)\n",
    "\n",
    "  # We first tokenize `orig_text`, strip whitespace from the result\n",
    "  # and `pred_text`, and check if they are the same length. If they are\n",
    "  # NOT the same length, the heuristic has failed. If they are the same\n",
    "  # length, we assume the characters are one-to-one aligned.\n",
    "  tokenizer = tokenization.BasicTokenizer(do_lower_case=do_lower_case)\n",
    "\n",
    "  tok_text = \" \".join(tokenizer.tokenize(orig_text))\n",
    "\n",
    "  start_position = tok_text.find(pred_text)\n",
    "  if start_position == -1:\n",
    "#     if FLAGS.verbose_logging:\n",
    "#       tf.logging.info(\n",
    "#           \"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n",
    "    return orig_text\n",
    "  end_position = start_position + len(pred_text) - 1\n",
    "\n",
    "  (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n",
    "  (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n",
    "\n",
    "  if len(orig_ns_text) != len(tok_ns_text):\n",
    "#     if FLAGS.verbose_logging:\n",
    "#       tf.logging.info(\"Length not equal after stripping spaces: '%s' vs '%s'\",\n",
    "#                       orig_ns_text, tok_ns_text)\n",
    "    return orig_text\n",
    "\n",
    "  # We then project the characters in `pred_text` back to `orig_text` using\n",
    "  # the character-to-character alignment.\n",
    "  tok_s_to_ns_map = {}\n",
    "  for (i, tok_index) in six.iteritems(tok_ns_to_s_map):\n",
    "    tok_s_to_ns_map[tok_index] = i\n",
    "\n",
    "  orig_start_position = None\n",
    "  if start_position in tok_s_to_ns_map:\n",
    "    ns_start_position = tok_s_to_ns_map[start_position]\n",
    "    if ns_start_position in orig_ns_to_s_map:\n",
    "      orig_start_position = orig_ns_to_s_map[ns_start_position]\n",
    "\n",
    "  if orig_start_position is None:\n",
    "#     if FLAGS.verbose_logging:\n",
    "#       tf.logging.info(\"Couldn't map start position\")\n",
    "    return orig_text\n",
    "\n",
    "  orig_end_position = None\n",
    "  if end_position in tok_s_to_ns_map:\n",
    "    ns_end_position = tok_s_to_ns_map[end_position]\n",
    "    if ns_end_position in orig_ns_to_s_map:\n",
    "      orig_end_position = orig_ns_to_s_map[ns_end_position]\n",
    "\n",
    "  if orig_end_position is None:\n",
    "#     if FLAGS.verbose_logging:\n",
    "#       tf.logging.info(\"Couldn't map end position\")\n",
    "    return orig_text\n",
    "\n",
    "  output_text = orig_text[orig_start_position:(orig_end_position + 1)]\n",
    "  return output_text\n",
    "\n",
    "\n",
    "def _get_best_indexes(logits, n_best_size):\n",
    "  \"\"\"Get the n-best logits from a list.\"\"\"\n",
    "  index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "  best_indexes = []\n",
    "  for i in range(len(index_and_score)):\n",
    "    if i >= n_best_size:\n",
    "      break\n",
    "    best_indexes.append(index_and_score[i][0])\n",
    "  return best_indexes\n",
    "\n",
    "\n",
    "def _compute_softmax(scores):\n",
    "  \"\"\"Compute softmax probability over raw logits.\"\"\"\n",
    "  if not scores:\n",
    "    return []\n",
    "\n",
    "  max_score = None\n",
    "  for score in scores:\n",
    "    if max_score is None or score > max_score:\n",
    "      max_score = score\n",
    "\n",
    "  exp_scores = []\n",
    "  total_sum = 0.0\n",
    "  for score in scores:\n",
    "    x = math.exp(score - max_score)\n",
    "    exp_scores.append(x)\n",
    "    total_sum += x\n",
    "\n",
    "  probs = []\n",
    "  for score in exp_scores:\n",
    "    probs.append(score / total_sum)\n",
    "  return probs\n",
    "\n",
    "\n",
    "class FeatureWriter(object):\n",
    "  \"\"\"Writes InputFeature to TF example file.\"\"\"\n",
    "\n",
    "  def __init__(self, filename, is_training):\n",
    "    self.filename = filename\n",
    "    self.is_training = is_training\n",
    "    self.num_features = 0\n",
    "    self._writer = tf.python_io.TFRecordWriter(filename)\n",
    "\n",
    "  def process_feature(self, feature):\n",
    "    \"\"\"Write a InputFeature to the TFRecordWriter as a tf.train.Example.\"\"\"\n",
    "    self.num_features += 1\n",
    "\n",
    "    def create_int_feature(values):\n",
    "      feature = tf.train.Feature(\n",
    "          int64_list=tf.train.Int64List(value=list(values)))\n",
    "      return feature\n",
    "\n",
    "    features = collections.OrderedDict()\n",
    "    features[\"unique_ids\"] = create_int_feature([feature.unique_id])\n",
    "    features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "    features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "    features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "\n",
    "    if self.is_training:\n",
    "      features[\"start_positions\"] = create_int_feature([feature.start_position])\n",
    "      features[\"end_positions\"] = create_int_feature([feature.end_position])\n",
    "      impossible = 0\n",
    "      if feature.is_impossible:\n",
    "        impossible = 1\n",
    "      features[\"is_impossible\"] = create_int_feature([impossible])\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "    self._writer.write(tf_example.SerializeToString())\n",
    "\n",
    "  def close(self):\n",
    "    self._writer.close()\n",
    "\n",
    "\n",
    "def validate_flags_or_throw(bert_config):\n",
    "  \"\"\"Validate the input FLAGS or throw an exception.\"\"\"\n",
    "  tokenization.validate_case_matches_checkpoint(FLAGS.do_lower_case,\n",
    "                                                FLAGS.init_checkpoint)\n",
    "\n",
    "  if not FLAGS.do_train and not FLAGS.do_predict:\n",
    "    raise ValueError(\"At least one of `do_train` or `do_predict` must be True.\")\n",
    "\n",
    "  if FLAGS.do_train:\n",
    "    if not FLAGS.train_file:\n",
    "      raise ValueError(\n",
    "          \"If `do_train` is True, then `train_file` must be specified.\")\n",
    "  if FLAGS.do_predict:\n",
    "    if not FLAGS.predict_file:\n",
    "      raise ValueError(\n",
    "          \"If `do_predict` is True, then `predict_file` must be specified.\")\n",
    "\n",
    "  if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n",
    "    raise ValueError(\n",
    "        \"Cannot use sequence length %d because the BERT model \"\n",
    "        \"was only trained up to sequence length %d\" %\n",
    "        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n",
    "\n",
    "  if FLAGS.max_seq_length <= FLAGS.max_query_length + 3:\n",
    "    raise ValueError(\n",
    "        \"The max_seq_length (%d) must be greater than max_query_length \"\n",
    "        \"(%d) + 3\" % (FLAGS.max_seq_length, FLAGS.max_query_length))\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def initializingModels():\n",
    "#     tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "    bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "    validate_flags_or_throw(bert_config)\n",
    "\n",
    "    tf.gfile.MakeDirs(FLAGS.output_dir)\n",
    "\n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "          vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "    tpu_cluster_resolver = None\n",
    "    if FLAGS.use_tpu and FLAGS.tpu_name:\n",
    "        tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "            FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
    "\n",
    "\n",
    "    is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "    run_config = tf.contrib.tpu.RunConfig(\n",
    "          cluster=tpu_cluster_resolver,\n",
    "          master=FLAGS.master,\n",
    "          model_dir=FLAGS.output_dir,\n",
    "          save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "          tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "              iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "              num_shards=FLAGS.num_tpu_cores,\n",
    "              per_host_input_for_training=is_per_host))\n",
    "    return bert_config, run_config, tokenizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_definition(bert_config, run_config):\n",
    "    train_examples = None\n",
    "    num_train_steps = None\n",
    "    num_warmup_steps = None\n",
    "    if FLAGS.do_train:\n",
    "        train_examples = read_squad_examples(\n",
    "            input_file=FLAGS.train_file, is_training=True)\n",
    "        num_train_steps = int(\n",
    "            len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "        num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "\n",
    "        # Pre-shuffle the input to avoid having to make a very large shuffle\n",
    "        # buffer in in the `input_fn`.\n",
    "        rng = random.Random(12345)\n",
    "        rng.shuffle(train_examples)\n",
    "\n",
    "    model_fn = model_fn_builder(\n",
    "          bert_config=bert_config,\n",
    "          init_checkpoint=FLAGS.init_checkpoint,\n",
    "          learning_rate=FLAGS.learning_rate,\n",
    "          num_train_steps=num_train_steps,\n",
    "          num_warmup_steps=num_warmup_steps,\n",
    "          use_tpu=FLAGS.use_tpu,\n",
    "          use_one_hot_embeddings=FLAGS.use_tpu)\n",
    "\n",
    "    # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "    # or GPU.\n",
    "    estimator = tf.contrib.tpu.TPUEstimator(\n",
    "          use_tpu=FLAGS.use_tpu,\n",
    "          model_fn=model_fn,\n",
    "          config=run_config,\n",
    "          train_batch_size=FLAGS.train_batch_size,\n",
    "          predict_batch_size=FLAGS.predict_batch_size)\n",
    "    return model_fn, estimator\n",
    "\n",
    "\n",
    "def testing_model(eval_examples, tokenizer, estimator):\n",
    "    eval_writer = FeatureWriter(\n",
    "        filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\n",
    "        is_training=False)\n",
    "    eval_features = []\n",
    "\n",
    "    def append_feature(feature):\n",
    "      eval_features.append(feature)\n",
    "      eval_writer.process_feature(feature)\n",
    "\n",
    "\n",
    "\n",
    "    convert_examples_to_features(\n",
    "        examples=eval_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=FLAGS.max_seq_length,\n",
    "        doc_stride=FLAGS.doc_stride,\n",
    "        max_query_length=FLAGS.max_query_length,\n",
    "        is_training=False,\n",
    "        output_fn=append_feature)\n",
    "    eval_writer.close()\n",
    "\n",
    "\n",
    "\n",
    "#     tf.logging.info(\"***** Running predictions *****\")\n",
    "#     tf.logging.info(\"  Num orig examples = %d\", len(eval_examples))\n",
    "#     tf.logging.info(\"  Num split examples = %d\", len(eval_features))\n",
    "#     tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    predict_input_fn = input_fn_builder(\n",
    "        input_file=eval_writer.filename,\n",
    "        seq_length=FLAGS.max_seq_length,\n",
    "        is_training=False,\n",
    "        drop_remainder=False)\n",
    "    \n",
    "    # If running eval on the TPU, you will need to specify the number of\n",
    "    # steps.\n",
    "    all_results = []\n",
    "    t1 = time.time()\n",
    "    for result in estimator.predict(\n",
    "        predict_input_fn, yield_single_examples=True):\n",
    "#       print(time.time() - t1)\n",
    "#       if len(all_results) % 1000 == 0:\n",
    "#         tf.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
    "      unique_id = int(result[\"unique_ids\"])\n",
    "      start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
    "      end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
    "      all_results.append(\n",
    "          RawResult(\n",
    "              unique_id=unique_id,\n",
    "              start_logits=start_logits,\n",
    "              end_logits=end_logits))\n",
    "    return eval_features, all_results\n",
    "\n",
    "def PredictAnswer(ContextSummary, question, tokenizer, estimator):\n",
    "    data = {'data': [{'title': 'Random',\n",
    "           'paragraphs': [{'context': ContextSummary,\n",
    "             'qas': [{'answers': [],\n",
    "               'question': question,\n",
    "               'id': '56be4db0acb8001400a502ec'}]}]}],\n",
    "         'version': '1.1'}\n",
    "    FLAGS.interact = True\n",
    "    eval_examples = read_squad_examples(data = data, is_training=False)\n",
    "    eval_features, all_results = testing_model(eval_examples, tokenizer, estimator)\n",
    "    output_prediction_file = os.path.join(FLAGS.output_dir, \"predictions.json\")\n",
    "    output_nbest_file = os.path.join(FLAGS.output_dir, \"nbest_predictions.json\")\n",
    "    output_null_log_odds_file = os.path.join(FLAGS.output_dir, \"null_odds.json\")\n",
    "    Prediction = write_predictions(eval_examples, eval_features, all_results,\n",
    "                      FLAGS.n_best_size, FLAGS.max_answer_length,\n",
    "                      FLAGS.do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file)\n",
    "    \n",
    "    return list(Prediction.values())[0]\n",
    "\n",
    "def main(_):\n",
    "    \n",
    "  if FLAGS.interact:\n",
    "    bert_config, run_config, tokenizer = initializingModels()\n",
    "    model_fn, estimator = model_definition(bert_config, run_config)\n",
    "    \n",
    "    def GetTextSummary(SummaryText, question):\n",
    "      S = SummaryText.apply(lambda x: max([word_vectors.wmdistance(remove_stopwords(sent).lower().split(),remove_stopwords(question).lower().split()) for sent in x]))\n",
    "      Summary = ' \\n\\n'.join(SummaryText[S.sort_values(ascending=False)[:2].index].apply(','.join).tolist())\n",
    "      return Summary\n",
    "\n",
    "#     with open(r'Data\\RPA_data_summary.txt') as f:\n",
    "#       text = f.read()\n",
    "    with open(FLAGS.context,'r') as file:\n",
    "      text = file.read()\n",
    "    SummaryText = Series(text.split('\\n\\n\\n'))\n",
    "    SummaryText = SummaryText.apply(lambda x:[i for i in sent_tokenize(x) if len(i)>2])\n",
    "    while True:\n",
    "      raw_text = input(\">>> \")\n",
    "      while not raw_text:\n",
    "        print('Prompt should not be empty!')\n",
    "        raw_text = input(\">>> \")\n",
    "      if raw_text.lower() == 'quit':\n",
    "        return\n",
    "      ContextSummary = GetTextSummary(SummaryText,raw_text)\n",
    "      out_text = PredictAnswer(ContextSummary,raw_text, tokenizer, estimator)\n",
    "      print(out_text)\n",
    "    \n",
    "  if FLAGS.question != None:\n",
    "    bert_config, run_config, tokenizer = initializingModels()\n",
    "    model_fn, estimator = model_definition(bert_config, run_config)\n",
    "    with open(FLAGS.context,'r') as file:\n",
    "      context = file.read()\n",
    "    Answer = PredictAnswer(SummaryText=context,question=FLAGS.question, tokenizer=tokenizer, estimator=estimator)\n",
    "    print(Answer)\n",
    "    return    \n",
    "    \n",
    "    \n",
    "  tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "  bert_config = modeling.BertConfig.from_json_file(FLAGS.bert_config_file)\n",
    "\n",
    "  validate_flags_or_throw(bert_config)\n",
    "\n",
    "  tf.gfile.MakeDirs(FLAGS.output_dir)\n",
    "\n",
    "  tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n",
    "\n",
    "  tpu_cluster_resolver = None\n",
    "  if FLAGS.use_tpu and FLAGS.tpu_name:\n",
    "    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n",
    "        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n",
    "\n",
    "  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n",
    "  run_config = tf.contrib.tpu.RunConfig(\n",
    "      cluster=tpu_cluster_resolver,\n",
    "      master=FLAGS.master,\n",
    "      model_dir=FLAGS.output_dir,\n",
    "      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "      tpu_config=tf.contrib.tpu.TPUConfig(\n",
    "          iterations_per_loop=FLAGS.iterations_per_loop,\n",
    "          num_shards=FLAGS.num_tpu_cores,\n",
    "          per_host_input_for_training=is_per_host))\n",
    "\n",
    "  train_examples = None\n",
    "  num_train_steps = None\n",
    "  num_warmup_steps = None\n",
    "  if FLAGS.do_train:\n",
    "    train_examples = read_squad_examples(\n",
    "        input_file=FLAGS.train_file, is_training=True)\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / FLAGS.train_batch_size * FLAGS.num_train_epochs)\n",
    "    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n",
    "\n",
    "    # Pre-shuffle the input to avoid having to make a very large shuffle\n",
    "    # buffer in in the `input_fn`.\n",
    "    rng = random.Random(12345)\n",
    "    rng.shuffle(train_examples)\n",
    "\n",
    "  model_fn = model_fn_builder(\n",
    "      bert_config=bert_config,\n",
    "      init_checkpoint=FLAGS.init_checkpoint,\n",
    "      learning_rate=FLAGS.learning_rate,\n",
    "      num_train_steps=num_train_steps,\n",
    "      num_warmup_steps=num_warmup_steps,\n",
    "      use_tpu=FLAGS.use_tpu,\n",
    "      use_one_hot_embeddings=FLAGS.use_tpu)\n",
    "\n",
    "  # If TPU is not available, this will fall back to normal Estimator on CPU\n",
    "  # or GPU.\n",
    "  estimator = tf.contrib.tpu.TPUEstimator(\n",
    "      use_tpu=FLAGS.use_tpu,\n",
    "      model_fn=model_fn,\n",
    "      config=run_config,\n",
    "      train_batch_size=FLAGS.train_batch_size,\n",
    "      predict_batch_size=FLAGS.predict_batch_size)\n",
    "\n",
    "  if FLAGS.do_train:\n",
    "    # We write to a temporary file to avoid storing very large constant tensors\n",
    "    # in memory.\n",
    "    train_writer = FeatureWriter(\n",
    "        filename=os.path.join(FLAGS.output_dir, \"train.tf_record\"),\n",
    "        is_training=True)\n",
    "    convert_examples_to_features(\n",
    "        examples=train_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=FLAGS.max_seq_length,\n",
    "        doc_stride=FLAGS.doc_stride,\n",
    "        max_query_length=FLAGS.max_query_length,\n",
    "        is_training=True,\n",
    "        output_fn=train_writer.process_feature)\n",
    "    train_writer.close()\n",
    "\n",
    "    tf.logging.info(\"***** Running training *****\")\n",
    "    tf.logging.info(\"  Num orig examples = %d\", len(train_examples))\n",
    "    tf.logging.info(\"  Num split examples = %d\", train_writer.num_features)\n",
    "    tf.logging.info(\"  Batch size = %d\", FLAGS.train_batch_size)\n",
    "    tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
    "    del train_examples\n",
    "\n",
    "    train_input_fn = input_fn_builder(\n",
    "        input_file=train_writer.filename,\n",
    "        seq_length=FLAGS.max_seq_length,\n",
    "        is_training=True,\n",
    "        drop_remainder=True)\n",
    "    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "\n",
    "  if FLAGS.do_predict:\n",
    "    eval_examples = read_squad_examples(\n",
    "        input_file=FLAGS.predict_file, is_training=False)\n",
    "\n",
    "    eval_writer = FeatureWriter(\n",
    "        filename=os.path.join(FLAGS.output_dir, \"eval.tf_record\"),\n",
    "        is_training=False)\n",
    "    eval_features = []\n",
    "\n",
    "    def append_feature(feature):\n",
    "      eval_features.append(feature)\n",
    "      eval_writer.process_feature(feature)\n",
    "\n",
    "    convert_examples_to_features(\n",
    "        examples=eval_examples,\n",
    "        tokenizer=tokenizer,\n",
    "        max_seq_length=FLAGS.max_seq_length,\n",
    "        doc_stride=FLAGS.doc_stride,\n",
    "        max_query_length=FLAGS.max_query_length,\n",
    "        is_training=False,\n",
    "        output_fn=append_feature)\n",
    "    eval_writer.close()\n",
    "\n",
    "    tf.logging.info(\"***** Running predictions *****\")\n",
    "    tf.logging.info(\"  Num orig examples = %d\", len(eval_examples))\n",
    "    tf.logging.info(\"  Num split examples = %d\", len(eval_features))\n",
    "    tf.logging.info(\"  Batch size = %d\", FLAGS.predict_batch_size)\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    predict_input_fn = input_fn_builder(\n",
    "        input_file=eval_writer.filename,\n",
    "        seq_length=FLAGS.max_seq_length,\n",
    "        is_training=False,\n",
    "        drop_remainder=False)\n",
    "\n",
    "    # If running eval on the TPU, you will need to specify the number of\n",
    "    # steps.\n",
    "    all_results = []\n",
    "    for result in estimator.predict(\n",
    "        predict_input_fn, yield_single_examples=True):\n",
    "      if len(all_results) % 1000 == 0:\n",
    "        tf.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
    "      unique_id = int(result[\"unique_ids\"])\n",
    "      start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
    "      end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
    "      all_results.append(\n",
    "          RawResult(\n",
    "              unique_id=unique_id,\n",
    "              start_logits=start_logits,\n",
    "              end_logits=end_logits))\n",
    "\n",
    "    output_prediction_file = os.path.join(FLAGS.output_dir, \"predictions.json\")\n",
    "    output_nbest_file = os.path.join(FLAGS.output_dir, \"nbest_predictions.json\")\n",
    "    output_null_log_odds_file = os.path.join(FLAGS.output_dir, \"null_odds.json\")\n",
    "\n",
    "    write_predictions(eval_examples, eval_features, all_results,\n",
    "                      FLAGS.n_best_size, FLAGS.max_answer_length,\n",
    "                      FLAGS.do_lower_case, output_prediction_file,\n",
    "                      output_nbest_file, output_null_log_odds_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  flags.mark_flag_as_required(\"vocab_file\")\n",
    "  flags.mark_flag_as_required(\"bert_config_file\")\n",
    "  flags.mark_flag_as_required(\"output_dir\")\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "FLAGS.interact =True\n",
    "FLAGS.context = r'Data\\delete.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> what are the benefits of rpa\n",
      "?,To understand why, consider that unidirectional models are efficiently trained by predicting each word conditioned on\n",
      ">>> what are the uses of bidirectional models\n",
      ", or BERT.,With this release, anyone in the world can train their own state-of-the\n",
      ">>> quit\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
